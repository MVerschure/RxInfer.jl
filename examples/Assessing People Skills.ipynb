{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing People’s Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/repos/RxInfer.jl/examples`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The project dependencies or compat requirements have changed since the manifest was last resolved.\n",
      "│ It is recommended to `Pkg.resolve()` or consider `Pkg.update()` if necessary.\n",
      "└ @ Pkg.API /Users/julia/.julia/scratchspaces/a66863c6-20e8-4ff4-8a62-49f30b1f605e/agent-cache/default-macmini-aarch64-4.0/build/default-macmini-aarch64-4-0/julialang/julia-release-1-dot-8/usr/share/julia/stdlib/v1.8/Pkg/src/API.jl:1535\n"
     ]
    }
   ],
   "source": [
    "# Activate local environment, see `Project.toml`\n",
    "import Pkg; Pkg.activate(\".\"); Pkg.instantiate(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this demo is to demonstrate the use of the `@node` and `@rule` macros, which allow a user to define custom factor nodes and associated update rules respectively. We will introduce these macros in the context of a root cause analysis on a student's test results. This demo is inspired by Chapter 2 of \"Model-Based Machine Learning\" by Winn et al.\n",
    "We consider a student who takes a test that consists of three questions. Answering each question correctly requires a combination of skills and attitude. More precisely, has the student studied for the test, and (since students like to party) have they slept the night before?\n",
    "We consider a model of binary variables. More precisely, we model the results for question $i$ as $r_i\\in\\{0,1\\}$, and root causes $s_i \\in \\{0, 1\\}$, where $s_1$ represents whether the student has slept or not, and $s_2$ and $s_3$ represent whether the student as studied the chapters for the corresponding questions.\n",
    "We assume the following model:\n",
    "- If the student as slept, they will be able to answer the first question;\n",
    "- If the student has slept or studied chapter two, then they will be able to answer question two;\n",
    "- If the student has answered question two correctly and studied chapter three, then they will be able to answer question three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using RxInfer, Random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRAPH HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RxInfer, the `@node` macro is utilized to depict a factor from Forney-style Factor Graph (FFG). This involves defining a function (referred to as a form) and the random variables, or edges, upon which the function operates in the FFG notation.\n",
    "\n",
    "There exist two types of edges in FFG notation: one for computing a posterior marginal, and the other called half-edges, which correspond to likelihoods in RxInfer. These half-edges are represented by constant PointMass marginals and do not require inference.\n",
    "\n",
    "In order to implement this functionality in RxInfer, one must specify a formtype identifier (function identifier), which in our case is a structure `AddNoise`, and provide a list of interfaces that correspond to the random variables in FFG notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AddNoise node\n",
    "struct AddNoise end\n",
    "\n",
    "@node AddNoise Stochastic [out, in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding update rule for AddNoise node\n",
    "@rule AddNoise(:in, Marginalisation) (q_out::PointMass,) = begin     \n",
    "    return Bernoulli(mean(q_out))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphPPL.jl export `@model` macro for model specification\n",
    "# It accepts a regular Julia function and builds an FFG under the hood\n",
    "@model function skill_model()\n",
    "\n",
    "    res = datavar(Float64, 3)\n",
    "\n",
    "    laziness ~ Bernoulli(0.5)\n",
    "    skill2 ~ Bernoulli(0.5)\n",
    "    skill3 ~ Bernoulli(0.5)\n",
    "\n",
    "    test1 ~ ¬laziness\n",
    "    test2 ~ ¬laziness -> skill2\n",
    "    test3 ~ test2 && skill3\n",
    "    \n",
    "    res[1] ~ AddNoise(test1)\n",
    "    res[2] ~ AddNoise(test2)\n",
    "    res[3] ~ AddNoise(test3)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that a student scoared $70\\%$ and $95\\%$ at first and second tests respectively. But got only $30\\%$ on the third one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inference results:\n",
       "  Posteriors       | available for (skill3, test2, test3, skill2, laziness)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_results = [0.7, 0.95, 0.3]\n",
    "\n",
    "inference_result = inference(\n",
    "    model = skill_model(),\n",
    "    data  = (res = test_results, )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bernoulli{Float64}(p=0.18704156479217607)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference_result.posteriors[:laziness]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bernoulli{Float64}(p=0.5806845965770171)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference_result.posteriors[:skill2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bernoulli{Float64}(p=0.3025672371638141)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference_result.posteriors[:skill3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results make sense. On the one hand, the student answered the first question correctly, which immediately gives us reason to believe that he is not lazy. He answered the second question pretty well, but this does not mean that the student had the skills to answer this question (attendance,i.e., lack of laziness, could help). To answer the third question, it was necessary to answer the second and have additional skills (#3). Unfortunately, the student's answer was weak, so our confidence about skill #3 was shattered."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
